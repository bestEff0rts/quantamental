---
title: "NAIVE BAYES classificator"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ISLR2)
attach(Smarket)
```

##1-Обучить модель Naive Bayes to Smarket data
```{r}
library(e1071)
train <- (Year < 2005)
Smarket.2005 <- Smarket[!train, ]
dim(Smarket.2005)
Direction.2005 <- Direction[!train]
nb.fit <-naiveBayes(Direction~Lag1 + Lag2, data = Smarket, subset = train)
nb.fit
```
Output Содержит оцененное mean значение и стандартное отклонение для каждой переменной в каждом классе
Так, mean for Lag1 is 0.04279022 for Direction=Down, sd=1.227446
По умолчанию, эта реализация модели байесовского классификатора позволяет получить количественный
признак, используя гауссово распределение. Однако для оценки распределений также может быть использован метод kernel density- плотности ядра.
```{r}
mean(Lag1[train][Direction[train] == "Down"])
sd(Lag1[train][Direction[train] == "Down"])
```
*predict()* на test data
```{r}
 nb.class <-predict(nb.fit, Smarket.2005)
 table(nb.class, Direction.2005)
```
Confusion matrix, TP=121, TN=28, TP Rate=121/(121+20)=0.858156; FP Rate= 83/(83+28)= 0.7477477
```{r}
mean(nb.class == Direction.2005)
```
Naive Bayes с этими данными дает точные прогнозы в течение В 59% случаев.
```{r}
 nb.preds <-predict(nb.fit, Smarket.2005, type = "raw")
 nb.preds[1:5, ]
```
Функция predict() также может генерировать оценки вероятности того, что каждое наблюдение относится к определенному классу.