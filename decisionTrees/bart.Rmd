---
title: "Bayesian Additive Regression Trees(bart)vs bagging vs RF"
output: github_document
---

```{r , include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ISLR2)
```

## Summary of Tree Ensemble Methods
*bagging*=bootstrapping+aggregating, sampling with repetition; деревья выращиваются независимо на основе случайных выборок наблюдений->деревья, как правило, очень похожи
друг на друга. Таким образом, создание пакетов-> может привести к локальным оптимумам и не
позволит полностью изучить пространство модели.

*random forests*= деревья выращиваются независимо на основе случайных выборок наблюдений; Однако каждый split (разбиение) на каждом дереве выполняется с использованием случайного подмножества объектов, тем самым
декорируя деревья -> более тщательное изучение пространства модели

*boosting*= используютя только исходные данные, никаких случайных выборок. Деревья выращиваются последовательно, используя подход “медленного”обучения: каждое новое дерево соответствует сигналу, оставшемуся от предыдущих деревьев, и shrunken down перед использованием.

*bart*= используем только исходные данные, последовательно выращиваем деревья. Однако каждое дерево is perutrbed(подвергается возмущению), чтобы избежать локальных минимумов и добиться более тщательного изучения пространства модели.

##BART
```{r setup}
attach(Boston)
library(BART)
```
# train test datasets
```{r}
set.seed(1)
train <-sample(1:nrow(Boston), nrow(Boston) / 2)
x <- Boston[, 1:12]
y <- Boston[, "medv"]

xtrain <- x[train, ]
ytrain <- y[train]
xtest <- x[-train, ]
ytest <- y[-train]
```
# fit bart
```{r}
bart.fit <-gbart(xtrain, ytrain, x.test = xtest)
```
#  test error
```{r}
yhat.bart <- bart.fit$yhat.test.mean
mean((ytest- yhat.bart)^2)
```
# проверим, сколько раз каждая переменная появлялась в коллекции деревьев.
```{r}
names(bart.fit)
ord <-order(bart.fit$varcount.mean, decreasing = T)
bart.fit$varcount.mean[ord]
```
# сравнение с  bagging(same data)
```{r}
library(randomForest)
set.seed(1)
bag.fit <-randomForest(medv ~ ., data = Boston,  subset = train, mtry = 12, importance = TRUE)
bag.fit
```
#bagging test set
```{r}
yhat.bag <-predict(bag.fit, newdata = Boston[-train, ])
boston.test <- Boston[-train, "medv"]
plot(yhat.bag, boston.test)
abline(0, 1)
print("test set MSE associated with the bagged regression tree=")
mean((yhat.bag- boston.test)^2)
```
# сравнение с random forests
```{r}
 rf.fit <-randomForest(medv~., data = Boston, subset = train, ntree = 25)
 yhat.bag <-predict(rf.fit, newdata = Boston[-train, ])
 mean((yhat.bag- boston.test)^2)
```
вывод: с этими данными лучше справляется bart 
bart test set mse=$15.97434$
bagging test set mse=$23.41916$
random forests test set mse= $19.76028$
